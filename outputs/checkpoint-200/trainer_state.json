{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0525978191148173,
  "eval_steps": 500,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010262989095574085,
      "grad_norm": 0.32396769523620605,
      "learning_rate": 2e-05,
      "loss": 2.2125,
      "step": 1
    },
    {
      "epoch": 0.02052597819114817,
      "grad_norm": 0.32879433035850525,
      "learning_rate": 4e-05,
      "loss": 2.2025,
      "step": 2
    },
    {
      "epoch": 0.030788967286722257,
      "grad_norm": 0.32992079854011536,
      "learning_rate": 6e-05,
      "loss": 2.2014,
      "step": 3
    },
    {
      "epoch": 0.04105195638229634,
      "grad_norm": 0.35406363010406494,
      "learning_rate": 8e-05,
      "loss": 2.1832,
      "step": 4
    },
    {
      "epoch": 0.05131494547787043,
      "grad_norm": 0.30759915709495544,
      "learning_rate": 0.0001,
      "loss": 2.1743,
      "step": 5
    },
    {
      "epoch": 0.061577934573444515,
      "grad_norm": 0.25657016038894653,
      "learning_rate": 0.00012,
      "loss": 2.1419,
      "step": 6
    },
    {
      "epoch": 0.0718409236690186,
      "grad_norm": 0.2514781057834625,
      "learning_rate": 0.00014,
      "loss": 2.0919,
      "step": 7
    },
    {
      "epoch": 0.08210391276459268,
      "grad_norm": 0.295661985874176,
      "learning_rate": 0.00016,
      "loss": 2.0843,
      "step": 8
    },
    {
      "epoch": 0.09236690186016677,
      "grad_norm": 0.19203561544418335,
      "learning_rate": 0.00018,
      "loss": 2.0677,
      "step": 9
    },
    {
      "epoch": 0.10262989095574086,
      "grad_norm": 0.22836489975452423,
      "learning_rate": 0.0002,
      "loss": 2.0658,
      "step": 10
    },
    {
      "epoch": 0.11289288005131494,
      "grad_norm": 0.1634054332971573,
      "learning_rate": 0.0001999863304992469,
      "loss": 2.035,
      "step": 11
    },
    {
      "epoch": 0.12315586914688903,
      "grad_norm": 0.4602564573287964,
      "learning_rate": 0.00019994532573409262,
      "loss": 2.0668,
      "step": 12
    },
    {
      "epoch": 0.1334188582424631,
      "grad_norm": 0.2848740518093109,
      "learning_rate": 0.00019987699691483048,
      "loss": 2.0456,
      "step": 13
    },
    {
      "epoch": 0.1436818473380372,
      "grad_norm": 0.4871521592140198,
      "learning_rate": 0.00019978136272187747,
      "loss": 2.0463,
      "step": 14
    },
    {
      "epoch": 0.1539448364336113,
      "grad_norm": 0.2330949604511261,
      "learning_rate": 0.000199658449300667,
      "loss": 2.0554,
      "step": 15
    },
    {
      "epoch": 0.16420782552918536,
      "grad_norm": 0.39611589908599854,
      "learning_rate": 0.00019950829025450114,
      "loss": 2.0215,
      "step": 16
    },
    {
      "epoch": 0.17447081462475947,
      "grad_norm": 0.3188719153404236,
      "learning_rate": 0.00019933092663536382,
      "loss": 2.0423,
      "step": 17
    },
    {
      "epoch": 0.18473380372033354,
      "grad_norm": 0.22723223268985748,
      "learning_rate": 0.00019912640693269752,
      "loss": 2.0089,
      "step": 18
    },
    {
      "epoch": 0.19499679281590762,
      "grad_norm": 0.2675345838069916,
      "learning_rate": 0.00019889478706014687,
      "loss": 2.0174,
      "step": 19
    },
    {
      "epoch": 0.20525978191148173,
      "grad_norm": 0.2426586002111435,
      "learning_rate": 0.00019863613034027224,
      "loss": 2.0058,
      "step": 20
    },
    {
      "epoch": 0.2155227710070558,
      "grad_norm": 0.26532992720603943,
      "learning_rate": 0.00019835050748723824,
      "loss": 2.0297,
      "step": 21
    },
    {
      "epoch": 0.22578576010262988,
      "grad_norm": 0.31806454062461853,
      "learning_rate": 0.00019803799658748094,
      "loss": 2.0058,
      "step": 22
    },
    {
      "epoch": 0.23604874919820398,
      "grad_norm": 0.53913813829422,
      "learning_rate": 0.00019769868307835994,
      "loss": 1.9537,
      "step": 23
    },
    {
      "epoch": 0.24631173829377806,
      "grad_norm": 0.8883881568908691,
      "learning_rate": 0.0001973326597248006,
      "loss": 1.9743,
      "step": 24
    },
    {
      "epoch": 0.25657472738935216,
      "grad_norm": 0.5930989384651184,
      "learning_rate": 0.00019694002659393305,
      "loss": 1.9124,
      "step": 25
    },
    {
      "epoch": 0.2668377164849262,
      "grad_norm": 0.7369909286499023,
      "learning_rate": 0.00019652089102773488,
      "loss": 1.8449,
      "step": 26
    },
    {
      "epoch": 0.2771007055805003,
      "grad_norm": 0.6618539094924927,
      "learning_rate": 0.00019607536761368484,
      "loss": 1.7982,
      "step": 27
    },
    {
      "epoch": 0.2873636946760744,
      "grad_norm": 0.8197206854820251,
      "learning_rate": 0.00019560357815343577,
      "loss": 1.7313,
      "step": 28
    },
    {
      "epoch": 0.29762668377164847,
      "grad_norm": 1.3301793336868286,
      "learning_rate": 0.00019510565162951537,
      "loss": 1.6867,
      "step": 29
    },
    {
      "epoch": 0.3078896728672226,
      "grad_norm": 0.6858351826667786,
      "learning_rate": 0.00019458172417006347,
      "loss": 1.6634,
      "step": 30
    },
    {
      "epoch": 0.3181526619627967,
      "grad_norm": 0.6292210817337036,
      "learning_rate": 0.00019403193901161613,
      "loss": 1.6231,
      "step": 31
    },
    {
      "epoch": 0.32841565105837073,
      "grad_norm": 0.8470697999000549,
      "learning_rate": 0.0001934564464599461,
      "loss": 1.6125,
      "step": 32
    },
    {
      "epoch": 0.33867864015394483,
      "grad_norm": 0.9799752235412598,
      "learning_rate": 0.00019285540384897073,
      "loss": 1.6226,
      "step": 33
    },
    {
      "epoch": 0.34894162924951894,
      "grad_norm": 0.7480965256690979,
      "learning_rate": 0.00019222897549773848,
      "loss": 1.6487,
      "step": 34
    },
    {
      "epoch": 0.359204618345093,
      "grad_norm": 0.8693040609359741,
      "learning_rate": 0.00019157733266550575,
      "loss": 1.6037,
      "step": 35
    },
    {
      "epoch": 0.3694676074406671,
      "grad_norm": 1.202073097229004,
      "learning_rate": 0.00019090065350491626,
      "loss": 1.54,
      "step": 36
    },
    {
      "epoch": 0.3797305965362412,
      "grad_norm": 1.279158353805542,
      "learning_rate": 0.00019019912301329592,
      "loss": 1.4981,
      "step": 37
    },
    {
      "epoch": 0.38999358563181524,
      "grad_norm": 1.1947357654571533,
      "learning_rate": 0.00018947293298207635,
      "loss": 1.6033,
      "step": 38
    },
    {
      "epoch": 0.40025657472738935,
      "grad_norm": 2.84165620803833,
      "learning_rate": 0.0001887222819443612,
      "loss": 1.6016,
      "step": 39
    },
    {
      "epoch": 0.41051956382296345,
      "grad_norm": 1.988013744354248,
      "learning_rate": 0.0001879473751206489,
      "loss": 1.6606,
      "step": 40
    },
    {
      "epoch": 0.4207825529185375,
      "grad_norm": 0.9505801200866699,
      "learning_rate": 0.00018714842436272773,
      "loss": 1.5682,
      "step": 41
    },
    {
      "epoch": 0.4310455420141116,
      "grad_norm": 0.9905043840408325,
      "learning_rate": 0.00018632564809575742,
      "loss": 1.5345,
      "step": 42
    },
    {
      "epoch": 0.4413085311096857,
      "grad_norm": 1.103534460067749,
      "learning_rate": 0.0001854792712585539,
      "loss": 1.5693,
      "step": 43
    },
    {
      "epoch": 0.45157152020525976,
      "grad_norm": 2.6128079891204834,
      "learning_rate": 0.00018460952524209355,
      "loss": 1.3983,
      "step": 44
    },
    {
      "epoch": 0.46183450930083386,
      "grad_norm": 2.393507957458496,
      "learning_rate": 0.00018371664782625287,
      "loss": 1.3719,
      "step": 45
    },
    {
      "epoch": 0.47209749839640797,
      "grad_norm": 1.3087619543075562,
      "learning_rate": 0.00018280088311480201,
      "loss": 1.3718,
      "step": 46
    },
    {
      "epoch": 0.482360487491982,
      "grad_norm": 1.3949534893035889,
      "learning_rate": 0.00018186248146866927,
      "loss": 1.37,
      "step": 47
    },
    {
      "epoch": 0.4926234765875561,
      "grad_norm": 1.317268967628479,
      "learning_rate": 0.00018090169943749476,
      "loss": 1.3132,
      "step": 48
    },
    {
      "epoch": 0.5028864656831302,
      "grad_norm": 2.073967456817627,
      "learning_rate": 0.0001799187996894925,
      "loss": 1.3812,
      "step": 49
    },
    {
      "epoch": 0.5131494547787043,
      "grad_norm": 1.343766450881958,
      "learning_rate": 0.00017891405093963938,
      "loss": 1.289,
      "step": 50
    },
    {
      "epoch": 0.5234124438742784,
      "grad_norm": 1.4530267715454102,
      "learning_rate": 0.00017788772787621126,
      "loss": 1.1553,
      "step": 51
    },
    {
      "epoch": 0.5336754329698524,
      "grad_norm": 2.0494415760040283,
      "learning_rate": 0.00017684011108568592,
      "loss": 1.2248,
      "step": 52
    },
    {
      "epoch": 0.5439384220654265,
      "grad_norm": 1.7045037746429443,
      "learning_rate": 0.0001757714869760335,
      "loss": 1.2377,
      "step": 53
    },
    {
      "epoch": 0.5542014111610006,
      "grad_norm": 1.4727474451065063,
      "learning_rate": 0.0001746821476984154,
      "loss": 1.2047,
      "step": 54
    },
    {
      "epoch": 0.5644644002565747,
      "grad_norm": 1.635600209236145,
      "learning_rate": 0.00017357239106731317,
      "loss": 1.1699,
      "step": 55
    },
    {
      "epoch": 0.5747273893521488,
      "grad_norm": 1.5954198837280273,
      "learning_rate": 0.00017244252047910892,
      "loss": 1.1785,
      "step": 56
    },
    {
      "epoch": 0.584990378447723,
      "grad_norm": 1.8886926174163818,
      "learning_rate": 0.00017129284482913972,
      "loss": 1.1069,
      "step": 57
    },
    {
      "epoch": 0.5952533675432969,
      "grad_norm": 1.5854507684707642,
      "learning_rate": 0.00017012367842724887,
      "loss": 0.9628,
      "step": 58
    },
    {
      "epoch": 0.605516356638871,
      "grad_norm": 1.4928507804870605,
      "learning_rate": 0.0001689353409118566,
      "loss": 1.0258,
      "step": 59
    },
    {
      "epoch": 0.6157793457344451,
      "grad_norm": 1.544190526008606,
      "learning_rate": 0.00016772815716257412,
      "loss": 1.1036,
      "step": 60
    },
    {
      "epoch": 0.6260423348300193,
      "grad_norm": 1.7023741006851196,
      "learning_rate": 0.0001665024572113848,
      "loss": 1.0172,
      "step": 61
    },
    {
      "epoch": 0.6363053239255934,
      "grad_norm": 2.3695030212402344,
      "learning_rate": 0.00016525857615241687,
      "loss": 1.1172,
      "step": 62
    },
    {
      "epoch": 0.6465683130211675,
      "grad_norm": 1.6469545364379883,
      "learning_rate": 0.00016399685405033167,
      "loss": 1.0243,
      "step": 63
    },
    {
      "epoch": 0.6568313021167415,
      "grad_norm": 1.5939934253692627,
      "learning_rate": 0.0001627176358473537,
      "loss": 0.8671,
      "step": 64
    },
    {
      "epoch": 0.6670942912123156,
      "grad_norm": 2.0975022315979004,
      "learning_rate": 0.0001614212712689668,
      "loss": 1.0494,
      "step": 65
    },
    {
      "epoch": 0.6773572803078897,
      "grad_norm": 2.0207784175872803,
      "learning_rate": 0.00016010811472830252,
      "loss": 0.9066,
      "step": 66
    },
    {
      "epoch": 0.6876202694034638,
      "grad_norm": 2.39949107170105,
      "learning_rate": 0.00015877852522924732,
      "loss": 0.887,
      "step": 67
    },
    {
      "epoch": 0.6978832584990379,
      "grad_norm": 1.7221235036849976,
      "learning_rate": 0.00015743286626829437,
      "loss": 0.9719,
      "step": 68
    },
    {
      "epoch": 0.708146247594612,
      "grad_norm": 1.750783085823059,
      "learning_rate": 0.0001560715057351673,
      "loss": 0.9116,
      "step": 69
    },
    {
      "epoch": 0.718409236690186,
      "grad_norm": 1.7083975076675415,
      "learning_rate": 0.00015469481581224272,
      "loss": 0.9145,
      "step": 70
    },
    {
      "epoch": 0.7286722257857601,
      "grad_norm": 1.8100048303604126,
      "learning_rate": 0.0001533031728727994,
      "loss": 0.8731,
      "step": 71
    },
    {
      "epoch": 0.7389352148813342,
      "grad_norm": 2.2551591396331787,
      "learning_rate": 0.00015189695737812152,
      "loss": 0.8814,
      "step": 72
    },
    {
      "epoch": 0.7491982039769083,
      "grad_norm": 2.650338888168335,
      "learning_rate": 0.0001504765537734844,
      "loss": 0.8153,
      "step": 73
    },
    {
      "epoch": 0.7594611930724824,
      "grad_norm": 2.109532594680786,
      "learning_rate": 0.00014904235038305083,
      "loss": 0.8772,
      "step": 74
    },
    {
      "epoch": 0.7697241821680565,
      "grad_norm": 1.6360656023025513,
      "learning_rate": 0.00014759473930370736,
      "loss": 0.792,
      "step": 75
    },
    {
      "epoch": 0.7799871712636305,
      "grad_norm": 1.7771669626235962,
      "learning_rate": 0.0001461341162978688,
      "loss": 0.9833,
      "step": 76
    },
    {
      "epoch": 0.7902501603592046,
      "grad_norm": 1.636940836906433,
      "learning_rate": 0.00014466088068528068,
      "loss": 0.8287,
      "step": 77
    },
    {
      "epoch": 0.8005131494547787,
      "grad_norm": 1.8404614925384521,
      "learning_rate": 0.00014317543523384928,
      "loss": 0.757,
      "step": 78
    },
    {
      "epoch": 0.8107761385503528,
      "grad_norm": 2.1086809635162354,
      "learning_rate": 0.00014167818604952906,
      "loss": 0.8109,
      "step": 79
    },
    {
      "epoch": 0.8210391276459269,
      "grad_norm": 1.878738522529602,
      "learning_rate": 0.00014016954246529696,
      "loss": 0.7502,
      "step": 80
    },
    {
      "epoch": 0.831302116741501,
      "grad_norm": 2.0604875087738037,
      "learning_rate": 0.00013864991692924523,
      "loss": 0.8663,
      "step": 81
    },
    {
      "epoch": 0.841565105837075,
      "grad_norm": 1.8194360733032227,
      "learning_rate": 0.00013711972489182208,
      "loss": 0.8575,
      "step": 82
    },
    {
      "epoch": 0.8518280949326491,
      "grad_norm": 1.528098464012146,
      "learning_rate": 0.00013557938469225167,
      "loss": 0.7529,
      "step": 83
    },
    {
      "epoch": 0.8620910840282232,
      "grad_norm": 1.5183902978897095,
      "learning_rate": 0.00013402931744416433,
      "loss": 0.67,
      "step": 84
    },
    {
      "epoch": 0.8723540731237973,
      "grad_norm": 1.6012190580368042,
      "learning_rate": 0.00013246994692046836,
      "loss": 0.7735,
      "step": 85
    },
    {
      "epoch": 0.8826170622193714,
      "grad_norm": 1.971519947052002,
      "learning_rate": 0.00013090169943749476,
      "loss": 0.7668,
      "step": 86
    },
    {
      "epoch": 0.8928800513149455,
      "grad_norm": 2.097022294998169,
      "learning_rate": 0.0001293250037384465,
      "loss": 0.8002,
      "step": 87
    },
    {
      "epoch": 0.9031430404105195,
      "grad_norm": 2.0760016441345215,
      "learning_rate": 0.00012774029087618446,
      "loss": 0.7694,
      "step": 88
    },
    {
      "epoch": 0.9134060295060936,
      "grad_norm": 1.7496371269226074,
      "learning_rate": 0.00012614799409538198,
      "loss": 0.6901,
      "step": 89
    },
    {
      "epoch": 0.9236690186016677,
      "grad_norm": 1.6072040796279907,
      "learning_rate": 0.00012454854871407994,
      "loss": 0.7081,
      "step": 90
    },
    {
      "epoch": 0.9339320076972418,
      "grad_norm": 1.4965510368347168,
      "learning_rate": 0.00012294239200467516,
      "loss": 0.6416,
      "step": 91
    },
    {
      "epoch": 0.9441949967928159,
      "grad_norm": 1.8970938920974731,
      "learning_rate": 0.0001213299630743747,
      "loss": 0.6901,
      "step": 92
    },
    {
      "epoch": 0.95445798588839,
      "grad_norm": 1.8893910646438599,
      "learning_rate": 0.00011971170274514802,
      "loss": 0.6959,
      "step": 93
    },
    {
      "epoch": 0.964720974983964,
      "grad_norm": 1.7603089809417725,
      "learning_rate": 0.000118088053433211,
      "loss": 0.7332,
      "step": 94
    },
    {
      "epoch": 0.9749839640795381,
      "grad_norm": 1.471929907798767,
      "learning_rate": 0.00011645945902807341,
      "loss": 0.5579,
      "step": 95
    },
    {
      "epoch": 0.9852469531751122,
      "grad_norm": 1.8713349103927612,
      "learning_rate": 0.0001148263647711842,
      "loss": 0.6911,
      "step": 96
    },
    {
      "epoch": 0.9955099422706863,
      "grad_norm": 1.5955501794815063,
      "learning_rate": 0.00011318921713420691,
      "loss": 0.6296,
      "step": 97
    },
    {
      "epoch": 1.0057729313662604,
      "grad_norm": 1.5753600597381592,
      "learning_rate": 0.00011154846369695863,
      "loss": 0.567,
      "step": 98
    },
    {
      "epoch": 1.0160359204618346,
      "grad_norm": 1.6767041683197021,
      "learning_rate": 0.0001099045530250463,
      "loss": 0.563,
      "step": 99
    },
    {
      "epoch": 1.0262989095574087,
      "grad_norm": 1.825105905532837,
      "learning_rate": 0.00010825793454723325,
      "loss": 0.6071,
      "step": 100
    },
    {
      "epoch": 1.0365618986529828,
      "grad_norm": 1.4973206520080566,
      "learning_rate": 0.00010660905843256994,
      "loss": 0.5231,
      "step": 101
    },
    {
      "epoch": 1.0468248877485569,
      "grad_norm": 1.5922938585281372,
      "learning_rate": 0.00010495837546732224,
      "loss": 0.5281,
      "step": 102
    },
    {
      "epoch": 1.0570878768441307,
      "grad_norm": 1.4724615812301636,
      "learning_rate": 0.00010330633693173082,
      "loss": 0.5108,
      "step": 103
    },
    {
      "epoch": 1.0673508659397049,
      "grad_norm": 1.7823505401611328,
      "learning_rate": 0.00010165339447663587,
      "loss": 0.5218,
      "step": 104
    },
    {
      "epoch": 1.077613855035279,
      "grad_norm": 2.2269680500030518,
      "learning_rate": 0.0001,
      "loss": 0.6956,
      "step": 105
    },
    {
      "epoch": 1.087876844130853,
      "grad_norm": 2.1415855884552,
      "learning_rate": 9.834660552336415e-05,
      "loss": 0.6198,
      "step": 106
    },
    {
      "epoch": 1.0981398332264272,
      "grad_norm": 1.78644597530365,
      "learning_rate": 9.669366306826919e-05,
      "loss": 0.4851,
      "step": 107
    },
    {
      "epoch": 1.1084028223220013,
      "grad_norm": 1.5643991231918335,
      "learning_rate": 9.504162453267777e-05,
      "loss": 0.4831,
      "step": 108
    },
    {
      "epoch": 1.1186658114175754,
      "grad_norm": 2.271282196044922,
      "learning_rate": 9.339094156743007e-05,
      "loss": 0.6134,
      "step": 109
    },
    {
      "epoch": 1.1289288005131495,
      "grad_norm": 2.0158703327178955,
      "learning_rate": 9.174206545276677e-05,
      "loss": 0.5245,
      "step": 110
    },
    {
      "epoch": 1.1391917896087236,
      "grad_norm": 1.9037480354309082,
      "learning_rate": 9.009544697495374e-05,
      "loss": 0.53,
      "step": 111
    },
    {
      "epoch": 1.1494547787042977,
      "grad_norm": 1.6869227886199951,
      "learning_rate": 8.845153630304139e-05,
      "loss": 0.4903,
      "step": 112
    },
    {
      "epoch": 1.1597177677998718,
      "grad_norm": 1.7561308145523071,
      "learning_rate": 8.681078286579311e-05,
      "loss": 0.514,
      "step": 113
    },
    {
      "epoch": 1.169980756895446,
      "grad_norm": 1.7110916376113892,
      "learning_rate": 8.517363522881579e-05,
      "loss": 0.488,
      "step": 114
    },
    {
      "epoch": 1.18024374599102,
      "grad_norm": 1.4939061403274536,
      "learning_rate": 8.35405409719266e-05,
      "loss": 0.4827,
      "step": 115
    },
    {
      "epoch": 1.1905067350865939,
      "grad_norm": 1.5112779140472412,
      "learning_rate": 8.191194656678904e-05,
      "loss": 0.4827,
      "step": 116
    },
    {
      "epoch": 1.200769724182168,
      "grad_norm": 1.4817100763320923,
      "learning_rate": 8.028829725485199e-05,
      "loss": 0.4383,
      "step": 117
    },
    {
      "epoch": 1.211032713277742,
      "grad_norm": 1.851823091506958,
      "learning_rate": 7.867003692562534e-05,
      "loss": 0.502,
      "step": 118
    },
    {
      "epoch": 1.2212957023733162,
      "grad_norm": 1.9697074890136719,
      "learning_rate": 7.705760799532485e-05,
      "loss": 0.473,
      "step": 119
    },
    {
      "epoch": 1.2315586914688903,
      "grad_norm": 1.8953315019607544,
      "learning_rate": 7.54514512859201e-05,
      "loss": 0.5104,
      "step": 120
    },
    {
      "epoch": 1.2418216805644644,
      "grad_norm": 1.5302566289901733,
      "learning_rate": 7.385200590461803e-05,
      "loss": 0.4509,
      "step": 121
    },
    {
      "epoch": 1.2520846696600385,
      "grad_norm": 1.6534475088119507,
      "learning_rate": 7.225970912381556e-05,
      "loss": 0.4605,
      "step": 122
    },
    {
      "epoch": 1.2623476587556126,
      "grad_norm": 1.6620733737945557,
      "learning_rate": 7.067499626155354e-05,
      "loss": 0.477,
      "step": 123
    },
    {
      "epoch": 1.2726106478511867,
      "grad_norm": 1.7726308107376099,
      "learning_rate": 6.909830056250527e-05,
      "loss": 0.4922,
      "step": 124
    },
    {
      "epoch": 1.2828736369467608,
      "grad_norm": 1.5798227787017822,
      "learning_rate": 6.753005307953167e-05,
      "loss": 0.4872,
      "step": 125
    },
    {
      "epoch": 1.293136626042335,
      "grad_norm": 1.3096177577972412,
      "learning_rate": 6.59706825558357e-05,
      "loss": 0.3865,
      "step": 126
    },
    {
      "epoch": 1.3033996151379088,
      "grad_norm": 1.4893139600753784,
      "learning_rate": 6.442061530774834e-05,
      "loss": 0.4365,
      "step": 127
    },
    {
      "epoch": 1.3136626042334831,
      "grad_norm": 1.5716071128845215,
      "learning_rate": 6.28802751081779e-05,
      "loss": 0.41,
      "step": 128
    },
    {
      "epoch": 1.323925593329057,
      "grad_norm": 1.9828070402145386,
      "learning_rate": 6.135008307075481e-05,
      "loss": 0.478,
      "step": 129
    },
    {
      "epoch": 1.3341885824246311,
      "grad_norm": 1.629611849784851,
      "learning_rate": 5.983045753470308e-05,
      "loss": 0.3823,
      "step": 130
    },
    {
      "epoch": 1.3444515715202052,
      "grad_norm": 1.8793628215789795,
      "learning_rate": 5.832181395047098e-05,
      "loss": 0.4685,
      "step": 131
    },
    {
      "epoch": 1.3547145606157793,
      "grad_norm": 1.4241970777511597,
      "learning_rate": 5.6824564766150726e-05,
      "loss": 0.3544,
      "step": 132
    },
    {
      "epoch": 1.3649775497113534,
      "grad_norm": 1.5904040336608887,
      "learning_rate": 5.533911931471936e-05,
      "loss": 0.4172,
      "step": 133
    },
    {
      "epoch": 1.3752405388069275,
      "grad_norm": 1.4455219507217407,
      "learning_rate": 5.386588370213124e-05,
      "loss": 0.4031,
      "step": 134
    },
    {
      "epoch": 1.3855035279025016,
      "grad_norm": 1.5041513442993164,
      "learning_rate": 5.240526069629265e-05,
      "loss": 0.3918,
      "step": 135
    },
    {
      "epoch": 1.3957665169980757,
      "grad_norm": 1.597215175628662,
      "learning_rate": 5.095764961694922e-05,
      "loss": 0.4141,
      "step": 136
    },
    {
      "epoch": 1.4060295060936499,
      "grad_norm": 1.7837165594100952,
      "learning_rate": 4.952344622651566e-05,
      "loss": 0.4519,
      "step": 137
    },
    {
      "epoch": 1.4162924951892237,
      "grad_norm": 1.6125749349594116,
      "learning_rate": 4.810304262187852e-05,
      "loss": 0.4205,
      "step": 138
    },
    {
      "epoch": 1.426555484284798,
      "grad_norm": 1.649978518486023,
      "learning_rate": 4.669682712720065e-05,
      "loss": 0.4015,
      "step": 139
    },
    {
      "epoch": 1.436818473380372,
      "grad_norm": 1.506298542022705,
      "learning_rate": 4.530518418775733e-05,
      "loss": 0.4055,
      "step": 140
    },
    {
      "epoch": 1.447081462475946,
      "grad_norm": 1.8072068691253662,
      "learning_rate": 4.392849426483274e-05,
      "loss": 0.4878,
      "step": 141
    },
    {
      "epoch": 1.4573444515715201,
      "grad_norm": 1.2165300846099854,
      "learning_rate": 4.256713373170564e-05,
      "loss": 0.3266,
      "step": 142
    },
    {
      "epoch": 1.4676074406670943,
      "grad_norm": 1.229669213294983,
      "learning_rate": 4.12214747707527e-05,
      "loss": 0.3221,
      "step": 143
    },
    {
      "epoch": 1.4778704297626684,
      "grad_norm": 1.4100066423416138,
      "learning_rate": 3.9891885271697496e-05,
      "loss": 0.3707,
      "step": 144
    },
    {
      "epoch": 1.4881334188582425,
      "grad_norm": 1.5220065116882324,
      "learning_rate": 3.857872873103322e-05,
      "loss": 0.3773,
      "step": 145
    },
    {
      "epoch": 1.4983964079538166,
      "grad_norm": 1.5527600049972534,
      "learning_rate": 3.7282364152646297e-05,
      "loss": 0.3923,
      "step": 146
    },
    {
      "epoch": 1.5086593970493907,
      "grad_norm": 1.72412109375,
      "learning_rate": 3.600314594966834e-05,
      "loss": 0.4232,
      "step": 147
    },
    {
      "epoch": 1.5189223861449648,
      "grad_norm": 1.172056794166565,
      "learning_rate": 3.4741423847583134e-05,
      "loss": 0.3337,
      "step": 148
    },
    {
      "epoch": 1.5291853752405387,
      "grad_norm": 1.3864123821258545,
      "learning_rate": 3.349754278861517e-05,
      "loss": 0.3808,
      "step": 149
    },
    {
      "epoch": 1.539448364336113,
      "grad_norm": 1.360754132270813,
      "learning_rate": 3.227184283742591e-05,
      "loss": 0.3875,
      "step": 150
    },
    {
      "epoch": 1.5497113534316869,
      "grad_norm": 1.608612060546875,
      "learning_rate": 3.106465908814342e-05,
      "loss": 0.398,
      "step": 151
    },
    {
      "epoch": 1.5599743425272612,
      "grad_norm": 1.3898682594299316,
      "learning_rate": 2.9876321572751144e-05,
      "loss": 0.3716,
      "step": 152
    },
    {
      "epoch": 1.570237331622835,
      "grad_norm": 1.300360083580017,
      "learning_rate": 2.87071551708603e-05,
      "loss": 0.3703,
      "step": 153
    },
    {
      "epoch": 1.5805003207184094,
      "grad_norm": 1.4925953149795532,
      "learning_rate": 2.7557479520891104e-05,
      "loss": 0.3428,
      "step": 154
    },
    {
      "epoch": 1.5907633098139833,
      "grad_norm": 1.230971336364746,
      "learning_rate": 2.6427608932686843e-05,
      "loss": 0.305,
      "step": 155
    },
    {
      "epoch": 1.6010262989095574,
      "grad_norm": 1.6369329690933228,
      "learning_rate": 2.5317852301584643e-05,
      "loss": 0.4053,
      "step": 156
    },
    {
      "epoch": 1.6112892880051315,
      "grad_norm": 1.4058692455291748,
      "learning_rate": 2.422851302396655e-05,
      "loss": 0.3262,
      "step": 157
    },
    {
      "epoch": 1.6215522771007056,
      "grad_norm": 1.8988932371139526,
      "learning_rate": 2.315988891431412e-05,
      "loss": 0.3912,
      "step": 158
    },
    {
      "epoch": 1.6318152661962797,
      "grad_norm": 1.6335623264312744,
      "learning_rate": 2.2112272123788768e-05,
      "loss": 0.3699,
      "step": 159
    },
    {
      "epoch": 1.6420782552918538,
      "grad_norm": 1.7481480836868286,
      "learning_rate": 2.1085949060360654e-05,
      "loss": 0.3861,
      "step": 160
    },
    {
      "epoch": 1.652341244387428,
      "grad_norm": 1.3565088510513306,
      "learning_rate": 2.008120031050753e-05,
      "loss": 0.3574,
      "step": 161
    },
    {
      "epoch": 1.6626042334830018,
      "grad_norm": 1.0882079601287842,
      "learning_rate": 1.9098300562505266e-05,
      "loss": 0.3084,
      "step": 162
    },
    {
      "epoch": 1.6728672225785761,
      "grad_norm": 1.2792365550994873,
      "learning_rate": 1.8137518531330767e-05,
      "loss": 0.3551,
      "step": 163
    },
    {
      "epoch": 1.68313021167415,
      "grad_norm": 1.5927265882492065,
      "learning_rate": 1.7199116885197995e-05,
      "loss": 0.3814,
      "step": 164
    },
    {
      "epoch": 1.6933932007697243,
      "grad_norm": 1.226210594177246,
      "learning_rate": 1.6283352173747145e-05,
      "loss": 0.3047,
      "step": 165
    },
    {
      "epoch": 1.7036561898652982,
      "grad_norm": 1.4590609073638916,
      "learning_rate": 1.5390474757906446e-05,
      "loss": 0.3626,
      "step": 166
    },
    {
      "epoch": 1.7139191789608723,
      "grad_norm": 1.464402437210083,
      "learning_rate": 1.4520728741446089e-05,
      "loss": 0.3698,
      "step": 167
    },
    {
      "epoch": 1.7241821680564464,
      "grad_norm": 1.5725802183151245,
      "learning_rate": 1.3674351904242611e-05,
      "loss": 0.3484,
      "step": 168
    },
    {
      "epoch": 1.7344451571520205,
      "grad_norm": 1.4907623529434204,
      "learning_rate": 1.2851575637272262e-05,
      "loss": 0.35,
      "step": 169
    },
    {
      "epoch": 1.7447081462475946,
      "grad_norm": 1.3077136278152466,
      "learning_rate": 1.2052624879351104e-05,
      "loss": 0.3062,
      "step": 170
    },
    {
      "epoch": 1.7549711353431687,
      "grad_norm": 1.279126524925232,
      "learning_rate": 1.1277718055638819e-05,
      "loss": 0.3032,
      "step": 171
    },
    {
      "epoch": 1.7652341244387428,
      "grad_norm": 1.318650245666504,
      "learning_rate": 1.0527067017923654e-05,
      "loss": 0.3394,
      "step": 172
    },
    {
      "epoch": 1.7754971135343167,
      "grad_norm": 1.4531922340393066,
      "learning_rate": 9.80087698670411e-06,
      "loss": 0.3628,
      "step": 173
    },
    {
      "epoch": 1.785760102629891,
      "grad_norm": 1.344150424003601,
      "learning_rate": 9.09934649508375e-06,
      "loss": 0.3278,
      "step": 174
    },
    {
      "epoch": 1.796023091725465,
      "grad_norm": 1.4839720726013184,
      "learning_rate": 8.422667334494249e-06,
      "loss": 0.3857,
      "step": 175
    },
    {
      "epoch": 1.8062860808210393,
      "grad_norm": 1.3261213302612305,
      "learning_rate": 7.771024502261526e-06,
      "loss": 0.3146,
      "step": 176
    },
    {
      "epoch": 1.8165490699166131,
      "grad_norm": 1.503188133239746,
      "learning_rate": 7.144596151029303e-06,
      "loss": 0.3793,
      "step": 177
    },
    {
      "epoch": 1.8268120590121875,
      "grad_norm": 1.390907645225525,
      "learning_rate": 6.543553540053926e-06,
      "loss": 0.3248,
      "step": 178
    },
    {
      "epoch": 1.8370750481077613,
      "grad_norm": 1.329161286354065,
      "learning_rate": 5.968060988383883e-06,
      "loss": 0.3199,
      "step": 179
    },
    {
      "epoch": 1.8473380372033354,
      "grad_norm": 1.5299756526947021,
      "learning_rate": 5.418275829936537e-06,
      "loss": 0.3239,
      "step": 180
    },
    {
      "epoch": 1.8576010262989096,
      "grad_norm": 1.3241322040557861,
      "learning_rate": 4.8943483704846475e-06,
      "loss": 0.3028,
      "step": 181
    },
    {
      "epoch": 1.8678640153944837,
      "grad_norm": 1.2755796909332275,
      "learning_rate": 4.3964218465642355e-06,
      "loss": 0.2945,
      "step": 182
    },
    {
      "epoch": 1.8781270044900578,
      "grad_norm": 1.096534252166748,
      "learning_rate": 3.924632386315186e-06,
      "loss": 0.2903,
      "step": 183
    },
    {
      "epoch": 1.8883899935856319,
      "grad_norm": 1.670883059501648,
      "learning_rate": 3.4791089722651436e-06,
      "loss": 0.3778,
      "step": 184
    },
    {
      "epoch": 1.898652982681206,
      "grad_norm": 1.049330472946167,
      "learning_rate": 3.059973406066963e-06,
      "loss": 0.2926,
      "step": 185
    },
    {
      "epoch": 1.9089159717767799,
      "grad_norm": 1.1687871217727661,
      "learning_rate": 2.667340275199426e-06,
      "loss": 0.2945,
      "step": 186
    },
    {
      "epoch": 1.9191789608723542,
      "grad_norm": 1.6428399085998535,
      "learning_rate": 2.3013169216400733e-06,
      "loss": 0.3547,
      "step": 187
    },
    {
      "epoch": 1.929441949967928,
      "grad_norm": 1.4776599407196045,
      "learning_rate": 1.9620034125190644e-06,
      "loss": 0.32,
      "step": 188
    },
    {
      "epoch": 1.9397049390635024,
      "grad_norm": 1.3405938148498535,
      "learning_rate": 1.6494925127617634e-06,
      "loss": 0.3347,
      "step": 189
    },
    {
      "epoch": 1.9499679281590763,
      "grad_norm": 1.3622108697891235,
      "learning_rate": 1.3638696597277679e-06,
      "loss": 0.3632,
      "step": 190
    },
    {
      "epoch": 1.9602309172546504,
      "grad_norm": 1.160789966583252,
      "learning_rate": 1.1052129398531507e-06,
      "loss": 0.2923,
      "step": 191
    },
    {
      "epoch": 1.9704939063502245,
      "grad_norm": 1.236912488937378,
      "learning_rate": 8.735930673024806e-07,
      "loss": 0.3201,
      "step": 192
    },
    {
      "epoch": 1.9807568954457986,
      "grad_norm": 1.3002899885177612,
      "learning_rate": 6.690733646361857e-07,
      "loss": 0.299,
      "step": 193
    },
    {
      "epoch": 1.9910198845413727,
      "grad_norm": 1.456615924835205,
      "learning_rate": 4.917097454988584e-07,
      "loss": 0.3457,
      "step": 194
    },
    {
      "epoch": 2.0012828736369466,
      "grad_norm": 1.330490231513977,
      "learning_rate": 3.415506993330153e-07,
      "loss": 0.3197,
      "step": 195
    },
    {
      "epoch": 2.011545862732521,
      "grad_norm": 1.175062894821167,
      "learning_rate": 2.1863727812254653e-07,
      "loss": 0.2923,
      "step": 196
    },
    {
      "epoch": 2.0218088518280948,
      "grad_norm": 0.8408181071281433,
      "learning_rate": 1.230030851695263e-07,
      "loss": 0.2349,
      "step": 197
    },
    {
      "epoch": 2.032071840923669,
      "grad_norm": 1.1788710355758667,
      "learning_rate": 5.467426590739511e-08,
      "loss": 0.3139,
      "step": 198
    },
    {
      "epoch": 2.042334830019243,
      "grad_norm": 1.1334446668624878,
      "learning_rate": 1.3669500753099585e-08,
      "loss": 0.2707,
      "step": 199
    },
    {
      "epoch": 2.0525978191148173,
      "grad_norm": 0.9751706123352051,
      "learning_rate": 0.0,
      "loss": 0.2588,
      "step": 200
    }
  ],
  "logging_steps": 1,
  "max_steps": 200,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7773291072087552.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
